# %%
import quandl
import pandas as pd

# %%
authtoken = 'CYmZJQTZk9cM-5LUZxD_'
def get_data_quandl(symbol, start_date, end_date):
    data = quandl.get(symbol, start_date=start_date,
                      end_date=end_date, authtoken=authtoken)
    return data

# %%
def generate_features(df):
    """Generate features for a stock/index based on historical price and performance
    Args:
        df(dataframe with columns "Open", "Close", "High", "Low", "Volume", "Adjusted Close")
    Returns:
        dataframe, data set with new features
        """
    df_new = pd.DataFrame()
    # 6 original features
    df_new['open'] = df['Open']
    df_new['open_1'] = df['Open'].shift(1)
    # Shift index by 1, in order to take the value of previous day. For example, [1, 3, 4, 2] -> [N/A, 1, 3, 4]
    df_new['close_1'] = df['Close'].shift(1)
    df_new['high_1'] = df['High'].shift(1)
    df_new['low_1'] = df['Low'].shift(1)
    df_new['volume_1'] = df['Volume'].shift(1)
    # 31 original features
    # average price
    df_new['avg_price_5'] = df['Close'].rolling(5).mean().shift(1)
    df_new['avg_price_30'] = df['Close'].rolling(21).mean().shift(1)
    df_new['avg_price_365'] = df['Close'].rolling(252).mean().shift(1)
    df_new['ratio_avg_price_5_30'] = df_new['avg_price_5'] / df_new['avg_price_30']
    df_new['ratio_avg_price_5_365'] = df_new['avg_price_5'] / df_new['avg_price_365']
    df_new['ratio_avg_price_30_365'] = df_new['avg_price_30'] / df_new['avg_price_365']
    # average volume
    df_new['avg_volume_5'] = df['Volume'].rolling(5).mean().shift(1)
    df_new['avg_volume_30'] = df['Volume'].rolling(21).mean().shift(1)
    df_new['avg_volume_365'] = df['Volume'].rolling(252).mean().shift(1)
    df_new['ratio_avg_volume_5_30'] = df_new['avg_volume_5'] / df_new['avg_volume_30']
    df_new['ratio_avg_volume_5_365'] = df_new['avg_volume_5'] / df_new['avg_volume_365']
    df_new['ratio_avg_volume_30_365'] = df_new['avg_volume_30'] / df_new['avg_volume_365']
    # standard deviation of prices
    df_new['std_price_5'] = df['Close'].rolling(5).std().shift(1)
    # rolling_mean calculates the moving standard deviation given a window
    df_new['std_price_30'] = df['Close'].rolling(21).std().shift(1)
    df_new['std_price_365'] = df['Close'].rolling(252).std().shift(1)
    df_new['ratio_std_price_5_30'] = df_new['std_price_5'] / df_new['std_price_30']
    df_new['ratio_std_price_5_365'] = df_new['std_price_5'] / df_new['std_price_365']
    df_new['ratio_std_price_30_365'] = df_new['std_price_30'] / df_new['std_price_365']
    # standard deviation of volumes
    df_new['std_volume_5'] = df['Volume'].rolling(5).std().shift(1)
    df_new['std_volume_30'] = df['Volume'].rolling(21).std().shift(1)
    df_new['std_volume_365'] = df['Volume'].rolling(252).std().shift(1)
    df_new['ratio_std_volume_5_30'] = df_new['std_volume_5'] / df_new['std_volume_30']
    df_new['ratio_std_volume_5_365'] = df_new['std_volume_5'] / df_new['std_volume_365']
    df_new['ratio_std_volume_30_365'] = df_new['std_volume_30'] / df_new['std_volume_365']
    # return
    df_new['return_1'] = ((df['Close'] - df['Close'].shift(1)) / df['Close'].shift(1)).shift(1)
    df_new['return_5'] = ((df['Close'] - df['Close'].shift(5)) / df['Close'].shift(5)).shift(1)
    df_new['return_30'] = ((df['Close'] - df['Close'].shift(21)) / df['Close'].shift(21)).shift(1)
    df_new['return_365'] = ((df['Close'] - df['Close'].shift(252)) / df['Close'].shift(252)).shift(1)
    df_new['moving_avg_5'] = df_new['return_1'].rolling(5).mean()
    df_new['moving_avg_30'] = df_new['return_1'].rolling(21).mean()
    df_new['moving_avg_365'] = df_new['return_1'].rolling(252).mean()
    # the target
    df_new['close'] = df['Close']
    df_new = df_new.dropna(axis=0)
    # This will drop rows with any N/A value, which is by-product of moving average/std.
    return df_new
# %%
#quandl.get_table('SHARADAR/SEP', ticker=['AAPL','TSLA'])
symbol = 'EOD/AAPL'
start = '1988-01-01'
end = '2015-12-31'
data_raw = get_data_quandl(symbol, start, end)
data = generate_features(data_raw)
# %%
def compute_prediction(X, weights):
    """ Compute the prediction y_hat based on current weights
    Args:
    X (numpy.ndarray)
    weights (numpy.ndarray)
    Returns:
    numpy.ndarray, y_hat of X under weights
    """
    predictions = np.dot(X, weights)
    return predictions
# %%
def update_weights_gd(X_train, y_train, weights, learning_rate):
    predictions = compute_prediction(X_train, weights)
    weights_delta = np.dot(X_train.T, y_train - predictions)
    m = y_train.shape[0]
    weights += learning_rate / float(m) * weights_delta
    return weights
# %%
def compute_cost(X, y, weights):
    predictions = compute_prediction(X, weights)
    cost = np.mean((predictions - y) ** 2 / 2.0)
    return cost
# %%
def train_linear_regression(X_train, y_train, max_iter, learning_rate, fit_intercept=False):
    if fit_intercept:
        intercept = np.ones((X_train.shape[0], 1))
        X_train = np.hstack((intercept, X_train))
        weights = np.zeros(X_train.shape[1])
        for iteration in range(max_iter):
            weights = update_weights_gd(X_train, y_train, weights, learning_rate)
            if iteration % 100 == 0:
                print(compute_cost(X_train, y_train, weights))
        return weights
# %%
def predict(X, weights):
    if X.shape[1] == weights.shape[0] - 1:
        intercept = np.ones((X.shape[0], 1))
        X = np.hstack((intercept, X))
    return compute_prediction(X, weights)
# %%
import numpy as np
X_train = np.array([[6], [2], [3], [4], [1],
                    [5], [2], [6], [4], [7]])
y_train = np.array([5.5, 1.6, 2.2, 3.7, 0.8,
                    5.2, 1.5, 5.3, 4.4, 6.8])
# %%
weights = train_linear_regression(X_train, y_train,
                                  max_iter=100, learning_rate=0.01, fit_intercept=True)
# %%
%matplotlib inline
X_test = np.array([[1.3], [3.5], [5.2], [2.8]])
predictions = predict(X_test, weights)
import matplotlib.pyplot as plt
plt.scatter(X_train[:, 0], y_train, marker='o', c='b')
plt.scatter(X_test[:, 0], predictions, marker='*', c='k')
plt.xlabel('x')
plt.ylabel('y')
plt.show()
# %%
from sklearn.linear_model import SGDRegressor
regressor = SGDRegressor(loss='squared_loss', penalty='l2',
                         alpha=0.0001, learning_rate='constant', eta0=0.01, max_iter=1000)
# %%
regressor.fit(X_train, y_train)
predictions = regressor.predict(X_test)
print(predictions)
# %%
import datetime
start_train = datetime.datetime(1988, 1, 1, 0, 0)
end_train = datetime.datetime(2014, 12, 31, 0, 0)
data_train = data.loc[start_train:end_train]
# %%
X_columns = list(data.drop(['close'], axis=1).columns)
y_column = 'close'
X_train = data_train[X_columns]
y_train = data_train[y_column]
# %%
print(X_train.shape)
y_train.shape
# %%
start_test = datetime.datetime(2015, 1, 1, 0, 0)
end_test = datetime.datetime(2015, 12, 31, 0, 0)
data_test = data.loc[start_test:end_test]
X_test = data_test[X_columns]
y_test = data_test[y_column]
# %%
X_test.shape
# %%
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
# %%
scaler.fit(X_train)
# %%
X_scaled_train = scaler.transform(X_train)
X_scaled_test = scaler.transform(X_test)
# %%
from sklearn.model_selection import GridSearchCV
param_grid = {"alpha": [3e-06, 1e-5, 3e-5],
              "eta0": [0.01, 0.03, 0.1],
             }
lr = SGDRegressor(penalty='l2', n_iter=1000)
grid_search = GridSearchCV(lr, param_grid, cv=5,
                           scoring='neg_mean_absolute_error')
grid_search.fit(X_scaled_train, y_train)
# %%
print(grid_search.best_params_)
lr_best = grid_search.best_estimator_
predictions_lr = lr_best.predict(X_scaled_test)
# %%
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
print('MSE: {0:.3f}'.format(mean_squared_error(y_test, predictions_lr)))
print('MAE: {0:.3f}'.format(mean_absolute_error(y_test, predictions_lr)))
print('R^2: {0:.3f}'.format(r2_score(y_test, predictions_lr)))

# %%
from sklearn.ensemble import RandomForestRegressor
param_grid = {"max_depth": [30, 50],
              "min_samples_split": [3, 5, 10],
             }
rf = RandomForestRegressor(n_estimators=1000)
grid_search = GridSearchCV(rf, param_grid, cv=5,
                           scoring='neg_mean_absolute_error')
grid_search.fit(X_train, y_train)
# %%
print(grid_search.best_params_)
rf_best = grid_search.best_estimator_
predictions_rf = rf_best.predict(X_test)
# %%
print('MSE: {0:.3f}'.format(mean_squared_error(y_test, predictions_rf)))
print('MAE: {0:.3f}'.format(mean_absolute_error(y_test, predictions_rf)))
print('R^2: {0:.3f}'.format(r2_score(y_test, predictions_rf)))

# %%
from sklearn.svm import SVR
param_grid = {"C": [1000, 3000, 10000],
              "epsilon": [0.00001, 0.00003, 0.0001],
             }
svr = SVR(kernel='linear')
grid_search = GridSearchCV(svr, param_grid, cv=5, scoring='neg_mean_absolute_error')
grid_search.fit(X_scaled_train, y_train)
print(grid_search.best_params_)
svr_best = grid_search.best_estimator_
predictions_svr = svr_best.predict(X_scaled_test)
print('MSE: {0:.3f}'.format(mean_squared_error(y_test, predictions_svr)))
print('MAE: {0:.3f}'.format(mean_absolute_error(y_test, predictions_svr)))
print('R^2: {0:.3f}'.format(r2_score(y_test, predictions_svr)))

# %%
import matplotlib.pyplot as plt

dates = data_test.index.values
plt.yscale('log')
plot_truth, = plt.plot(dates, y_test, 'k')
plot_lr, = plt.plot(dates, predictions_lr, 'r')
plot_rf, = plt.plot(dates, predictions_rf, 'b')
plot_svr, = plt.plot(dates, predictions_svr, 'g')
plt.legend([plot_truth, plot_lr, plot_rf, plot_svr], ['Truth', 'Linear regression', 'Random forest', 'SVR'])
plt.title('Stock price prediction vs truth')
plt.show()
# %%
